\section{Logistic Regression}

% {theorem}{Twierdzenie}[section]
% *{condition}{Kryterium}
% {corollary}{Wniosek}[theorem]
% {lemma}{Lemat}[theorem]
% {statement}{Stwierdzenie}[section]
% {problem}{Problem}[section]
% {question}{Pytanie}[section]
% *{definition}{Definicja}
% *{note}{Uwaga}


\begin{problem}[classification]
    For given n + 1 features, where first one is bias feature, execute binary classification.
\end{problem}
Solutions:
\begin{enumerate}
    \item Linear regression with threshold - doesn't work well when big and small values exist simultaneously.
    \item Logistic regression algorithm - works pretty well
    \item Neural Networks - more complicated than logistic regression
\end{enumerate}
\begin{definition}[sigmoid function]
    $g(x) = \frac{1}{1 + e^{-x}}$
\end{definition}
\begin{definition}[hypothesis]
    For given $\theta \in \mathbb{R}^{n+1}$ we define $h_\theta(x) = g(\theta^T x)$
\end{definition}

\begin{definition}[cost function]
    \begin{equation*}
        J(\theta) = \tfrac{1}{m}\sum\limits_{i=1}^{m} - y^{(i)} \log (h_\theta(x^{(i)}))  - ( 1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))
    \end{equation*}
\end{definition}

\begin{definition}[partial derivative]
    \begin{equation*}
        \frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}_j
    \end{equation*}
\end{definition}

The same partial derivative, but different cost function.

\begin{problem}[multiclass classification]
    Classification for multiple classes. For every case class create binary hypothesis - or neural networks.
\end{problem}

